# {{cookiecutter.project_name}}

![main](https://github.com/{{ cookiecutter.github_username }}/{{ cookiecutter.project_slug }}/actions/workflows/main.yml/badge.svg)
{% if cookiecutter.project_license != "No License" %}[![License {{ cookiecutter.project_license }}](https://img.shields.io/github/license/{{ cookiecutter.github_username }}/{{ cookiecutter.project_slug }})](./LICENSE){% endif %}


{{ cookiecutter.project_description}}


## Preparation
To reproduce the project, clone this repository on your machine
```bash
git clone https://github.com/{{ cookiecutter.github_username }}/{{ cookiecutter.project_slug }}
```
As a next step, although optional, I'd highly recommended that you create a new virtual environment.
Then, in the new, cloned directory (`cd {{ cookiecutter.project_slug }}/`) run
```
python setup.py install
```
to make the source code in `src` as a package available.<br>
Or, if you plan on making changes on the code, run
```
python setup.py develop
```
instead of the prior command. That way, any changes you make in the source code should be immediately reflected in the installed version and therefore be instantly available for imports.

Next, make the raw data available or accessible under `data/` (see project structure below).
If the project is dealing with large amounts of data that reside somewhere outside your home directory,
I would suggest that you link the respective subdirectories inside `data/` accordingly.
The python scripts should be able to follow symlinks.

<!-- If all is set up, you can run `make test_structure` to perform some tests before starting running the scripts or Jupyter notebooks in the respective directories. -->


## Usage
All scripts and Jupyter notebooks that deal with either processing of the data or the creation of any kind of reports (plots, documents, etc) are available in `scripts/` and `notebooks/`, respectively.<br>
Code residing in `src/` is _exclusively_ source code and is not actively executed.<br>
Both the scripts and the notebooks are named in a way that indicates their order of execution.
For standard tasks, you might find respective commands in the Makefile. Just type `make` to see a list of available commands.

<u>A recommendation for long-running tasks:</u><br>
Some tasks like data processing will need a long time. 
It is highly recommended that you use a detachable terminal environment like `screen` or `tmux`.
This way you can detach from the session (even close your terminal) without losing or ending the process.

## Project Structure

    ├── LICENSE            <- The license used for this project
    ├── CHANGELOG.md       <- All major changes should go in there
    ├── Makefile           <- A self-documenting Makefile for standard CLI tasks
    ├── README.md          <- The top-level README of this project
    ├── .env               <- In this file, specify all your custom environment variables
    │                         Keep this out of version control!
    │
    ├── assets             <- A place for assets like shapefiles or config files
    │
    ├── data               <- Contains all data used for the analyses in this project.
    │   │                     The sub-directories can be links to the actual location of your data.
    │   │                     However, they should never be under version control! (-> .gitignore)
    │   ├── interim        <- Intermediate data that have been transformed from the raw data
    │   ├── processed      <- The final, processed data used for the actual analyses
    │   └── raw            <- The original, immutable(!) data
    │
    ├── docs               <- The technical documentation of the project
    │                         A report of the actual project can be found in `reports/book`.
    │
    ├── logs               <- Storage location for the log files being generated by scripts
    │
    ├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
    │   │                     and a short `-` or `_` delimited description, e.g. `01-initial-analyses`
    │   ├── exploratory    <- Notebooks for exploratory tasks
    │   └── reports        <- Notebooks generating reports and figures
    │
    ├── references         <- Data descriptions, manuals, and all other explanatory materials
    │
    ├── reports            <- Generated reports (e.g. HTML, PDF, LaTeX, etc.)
    │   ├── book           <- A Jupyter-Book describing the project
    │   └── figures        <- Generated graphics and figures to be used in reporting
    │
    ├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    │                         generated with `pip freeze > requirements.txt`
    │
    ├── setup.py           <- makes project pip installable (`pip install -e .`) so src can be imported
    ├── scripts            <- High-level scripts that use (low-level) source code from `src/`
    └── src                <- Source code (and only source code!) for use in this project
        ├── tests          <- Contains tests for the code in `src/`
        └── __init__.py    <- Makes src a Python module and provides some standard variables


## Dummy files
The following files are for demonstration purposes only and, if not needed, can be deleted safely:

    ├── notebooks/01-minimal-example.ipynb
    │
    ├── reports/book/*
    │
    ├── scripts/01-test.py
    └── src
        ├── tests/*
        └── submodule.py


## Testing
To test the code, run `make tests` in the root directory.
This will execute both the unit tests and docstring examples using `pytest`.

<!-- Run `make coverage` to generate a test coverage report and `make lint` to check code style consistency. -->


## Features
* [ ] TODO


## Maintainer
- [markusritschel](https://github.com/markusritschel)


## Contact & Issues
For any questions or issues, please contact me via {{ cookiecutter.email }} or open an [issue](https://github.com/{{ cookiecutter.github_username }}/{{ cookiecutter.project_slug }}/issues).


---
&copy; {{ cookiecutter.project_author }} {% now 'local', '%Y' %}
