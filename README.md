# Cookiecutter PySci-project Template
![build](https://github.com/markusritschel/cookiecutter-pysci-project/actions/workflows/main.yml/badge.svg)
[![License MIT](https://img.shields.io/github/license/markusritschel/cookiecutter-pysci-project)](./LICENSE)

_Just another [CookieCutter](https://github.com/cookiecutter/cookiecutter) Template for Scientific Python Projects._

## What is it good for?
[CookieCutter](https://cookiecutter.readthedocs.io/) is a templating engine for creating directory structures including pre-defined files based on a question catalogue that is being asked during the setup.<br>
By running `cookiecutter` with this repository, a new directory will be created with a pre-defined structure and some basic files, making you all ready for starting a new scientific python project.

## About this template
There exist tons of different CookieCutter templates for all different kinds of projects.
However, according to my experience, many of them are very complex in their structure and therefore often a bit overkill, especially for new-comers or projects of a rather modest size.
<br>
This template provides a boilerplate for small to medium-size (scientific) data projects, e.g. a thesis, a group project, or similar.
For an overview of the structure have a look at the section further below.
The redundant parts (mainly for demonstration purposes) are only few and are listed in the section after the one describing the project structure.

Once set up, a git repository is automatically initialized. 
If you want to connect it with a remote repository on GitHub (or any other hosted git service) you need to add the respective remote repository to your local repository.

## Requirements
You need to have python installed as well as the python package `cookiecutter`.
You can do this either via pip or conda.
```bash
$ pip install -U cookiecutter
$ conda install -c conda-forge cookiecutter
```
Besides that, there is no need to clone or download anything from this repository. Just follow the next step :-)

## Usage
### Set up a new project
After having `cookiecutter` installed, create a new project from this template by executing one of the following commands:
```bash
$ cookiecutter gh:markusritschel/cookiecutter-pysci-project
$ cookiecutter https://github.com/markusritschel/cookiecutter-pysci-project.git
$ cookiecutter git+https://github.com/markusritschel/cookiecutter-pysci-project
$ cookiecutter git+ssh://git@github.com/markusritschel/cookiecutter-pysci-project.git
```
The script will ask you some questions based on the entries in the `cookiecutter.json` and will then create a new project based on this template with the information you have just given by answering the questions.

### Using the Makefile
The Makefile in the project directory provides some default routines like cleanup, testing, installing requirements etc.
<br>
Even though for many people using make seems to be a bit old-fashioned, I would recommend you making use of Make's great capability of dealing with dependencies.
This is in particular useful if, for example, the first step in your data-processing pipeline takes a long time to process your raw data and generate the interim product.
<br>
I usually structure my data-processing workflow such that I can run a single process via command line (for example `python scripts/process-raw-data.py -o ./output_dir`). `click`, `fire` and `docopt` provide neat functionalities to convert your scripts into interactive command-line interfaces.
These commands I can set as targets in the Makefile, for example:
```make
## Process raw data and write the newly generated data into ./data/interim/
process_raw_data:
    python scripts/process-raw-data.py
```
I can now simply run `make process_raw_data`.

#### Setting dependencies
Let's assume that the previous step (processing the raw data) generates new data inside `./data/interim/`. If I now have a second processing step that depends on the data generated by the previous step, I can set these data as dependencies for the new rule.
```make
## Process interim data
process_interim_data: $(wildcard data/interim/**/*)
    python scripts/process-interim-data.py
```
This way, the last step is only executed if the data it depends on have changed since the last time of execution.

For further information, have a look at Make's documentation: https://www.gnu.org/software/make/manual/html_node/Rules.html


## Project Structure

    ├── LICENSE            <- The license used for this project
    ├── CHANGELOG.md       <- All major changes should go in there
    ├── Makefile           <- A self-documenting Makefile for standard CLI tasks
    ├── README.md          <- The top-level README of this project
    ├── .env               <- In this file, specify all your custom environment variables
    │                         Keep this out of version control!
    │
    ├── assets             <- A place for assets like shapefiles or config files
    │
    ├── data               <- Contains all data used for the analyses in this project.
    │   │                     The sub-directories can be links to the actual location of your data.
    │   │                     However, they should never be under version control! (-> .gitignore)
    │   ├── interim        <- Intermediate data that have been transformed from the raw data
    │   ├── processed      <- The final, processed data used for the actual analyses
    │   └── raw            <- The original, immutable(!) data
    │
    ├── docs               <- The technical documentation (default engine: Jupyter-Book; but feel free to use 
    │                         Sphinx or MkDocs or anything similar)
    │                         This should contain only documentation of the code and the assets.
    │                         A report of the actual project should be placed in `reports/book`.
    │
    ├── logs               <- Storage location for the log files being generated by scripts
    │
    ├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
    │   │                     and a short `-` or `_` delimited description, e.g. `01-initial-analyses`
    │   ├── exploratory    <- Notebooks for exploratory tasks
    │   └── reports        <- Notebooks generating reports and figures
    │
    ├── references         <- Data descriptions, manuals, and all other explanatory materials
    │
    ├── reports            <- Generated reports (e.g. HTML, PDF, LaTeX, etc.)
    │   ├── book           <- A Jupyter-Book describing the project
    │   └── figures        <- Generated graphics and figures to be used in reporting
    │
    ├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    │                         generated with `pip freeze > requirements.txt`
    │
    ├── setup.py           <- makes project pip installable (pip install -e .) so src can be imported
    ├── scripts            <- High-level scripts that use (low-level) source code from `src/`
    └── src                <- Source code (and only source code!) for use in this project
        ├── tests          <- Contains tests for the code in `src/`
        └── __init__.py    <- Makes src a Python module and provides some standard variables


## Dummy files
The following files are for demonstration purposes only and, if not needed, can be deleted safely:

    ├── notebooks/01-minimal-example.ipynb
    ├── reports/book/*
    ├── scripts/01-test.py
    └── src
        ├── tests/*
        └── submodule.py


## Sources of inspiration
Some great sources of inspiration and orientation when I created this template:
- A great article on how to structure your scientific data projects: https://drivendata.github.io/cookiecutter-data-science
- https://github.com/drivendata/cookiecutter-data-science
- https://github.com/audreyfeldroy/cookiecutter-pypackage
- https://github.com/hackalog/easydata
- https://github.com/aubricus/cookiecutter-python-package


## Maintainer
- [Markus Ritschel](https://github.com/markusritschel)

## Contributing
Issues & pull-requests accepted.


---
&copy; Markus Ritschel 2021
