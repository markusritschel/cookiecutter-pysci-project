# Cookiecutter PySci-project Template
![build](https://github.com/markusritschel/cookiecutter-pysci-project/actions/workflows/main.yml/badge.svg)
[![License MIT](https://img.shields.io/github/license/markusritschel/cookiecutter-pysci-project)](./LICENSE)

_Just another [CookieCutter](https://github.com/cookiecutter/cookiecutter) Template for Scientific Python Projects._

## What is it good for?
[CookieCutter](https://cookiecutter.readthedocs.io/) is a templating engine for creating directory structures including pre-defined files based on a question catalogue that is being asked during the setup.<br>
By running `cookiecutter` with this repository, a new directory will be created with a pre-defined structure and some basic files, making you all ready for starting a new scientific python project.

## About this template
There exist tons of different CookieCutter templates for all different kinds of projects.
However, according to my experience, many of them are very complex in their structure and therefore often a bit overkill, especially for new-comers or projects of a rather modest size.
<br>
This template provides a boilerplate for small to medium-size (scientific) data projects, e.g. a thesis, a group project, or similar.
For an overview of the structure have a look at the section further below.
The redundant parts (mainly for demonstration purposes) are only few and are listed in the section after the one describing the project structure.

Once set up, a git repository is automatically initialized. 
If you want to connect it with a remote repository on GitHub (or any other hosted git service) you need to add the respective remote repository to your local repository.

## Requirements
You need to have python installed as well as the python package `cookiecutter`.
You can do this either via pip or conda.
```bash
$ pip install -U cookiecutter
$ conda install -c conda-forge cookiecutter
```
Besides that, there is no need to clone or download anything from this repository. Just follow the next step :-)

## Usage
### Set up a new project
After having `cookiecutter` installed, create a new project from this template by executing one of the following commands:
```bash
$ cookiecutter gh:markusritschel/cookiecutter-pysci-project
$ cookiecutter https://github.com/markusritschel/cookiecutter-pysci-project.git
$ cookiecutter git+https://github.com/markusritschel/cookiecutter-pysci-project
$ cookiecutter git+ssh://git@github.com/markusritschel/cookiecutter-pysci-project.git
```
The script will ask you some questions based on the entries in the `cookiecutter.json` and will then create a new project based on this template with the information you have just given by answering the questions.

### Using the Makefile
The Makefile in the project directory provides some default routines like cleanup, testing, installing requirements etc.
<br>
Even though for many people using make seems to be a bit old-fashioned, I would recommend you making use of Make's great capability of dealing with dependencies.
This is in particular useful if, for example, the first step in your data-processing pipeline takes a long time to process your raw data and generate the interim product.
<br>
I usually structure my data-processing workflow such that I can run a single process via command line (for example `python scripts/process-raw-data.py -o ./output_dir`). `click`, `fire` and `docopt` provide neat functionalities to convert your scripts into interactive command-line interfaces.
These commands I can set as targets in the Makefile, for example:
```make
## Process raw data and write the newly generated data into ./data/interim/
process_raw_data:
    python scripts/process-raw-data.py
```
I can now simply run `make process_raw_data`.

#### Setting dependencies
Let's assume that the previous step (processing the raw data) generates new data inside `./data/interim/`. If I now have a second processing step that depends on the data generated by the previous step, I can set these data as dependencies for the new rule.
```make
## Process interim data
process_interim_data: $(wildcard data/interim/**/*)
    python scripts/process-interim-data.py
```
This way, the last step is only executed if the data it depends on have changed since the last time of execution.

For further information, have a look at Make's documentation: https://www.gnu.org/software/make/manual/html_node/Rules.html

### Write your documentation
In my opinion it should be differentiated between two kinds of documentation: 
1. The first kind should only document your code (similar to what you would expect when opening the online documentation of a python package or similar) and should be considered as best practice to be shipped with your code.
2. The second is optional but, in my opinion, possibly very helpful for others (and also for yourself) to understand what is going on in your project.


For the first, I would recommend you to use [Sphinx](https://www.sphinx-doc.org/), which is particularly suited for documenting python code and already set up as default doc engine in this project template. It's _autodoc_ extension can also parse the doc strings of your code and process them to nice HTML output.

For the second purpose you can, in principle, use whichever tool you like the most (Sphinx, MkDocs, Jekyll, etc.). I personally like the [Jupyter Book](https://jupyterbook.org/) very much as it is feature-rich and you can use a bunch of languages (Jupyter Markdown, MyST Markdown for more publishing features, reStructuredText, even your Jupyter Notebooks, or any Jupytext format).

#### Using Sphinx
For a detailed description of how to use Sphinx and how to write your documentation check out their [website](https://www.sphinx-doc.org/).
In short: describe as much of your code as possible in the doc-strings of your functions, classes and modules.
Sphinx can then parse these doc-strings and format them nicely in your documentation output. 
To compile an HTML report of your Sphinx documentation, enter the `docsrc` directory and execute `make html`. Type `make` for more formats.
Alternatively you can run `make docs` from the root of your project.

##### Write documentation on a separate branch
I would suggest that you create a separate `docs` branch for writing your documentation to keep them separated from your code progress.
To write on your documentation, you would then always switch to the `docs` branch (remember to always merge your current code development branch to ensure Sphinx can parse the most up-to-date version).

##### Publish your documentation on [Github pages](https://pages.github.com/)
Github allows you to host static websites on their platform.
For this to work, you need to provide your HTML files in a `docs` directory located in the root of your project.
Add another branch `gh-pages` and add the following to the Makefile located under _docsrc/Makefile_.
```make
github:
    @make html
    @cp -a _build/html/. ../docs
```
On your `gh-pages` branch, by running `make github` from inside your `docsrc` directory, a `make html` is called first to create the HTML output of your documentation. 
Then, this output is copied to the `docs` directory in the root of your repository. 
This folder should only exist on the branch `gh-pages`.
On `gh-pages`, git add all files in `docs` directory and `git push` to the remote `gh-pages` branch.
On Github: select `gh-pages` as branch and `docs` as source for your page content.

#### Using Jupyter-Book
To compile your jupyter book, simply execute `jb build reports/book`.
Alternatively to your source code documentation, you can also place the content of your compiled jupyter book to `docs/` to publish it via Github pages.

#### Using both a report alongside your code documentation as Github page
Github pages allows only one website per repository. Usually that can be accessed via the domain https://your-github-usernam.github.io/your-project.
To use both your project html report (jupyter book) _and_ your technical code documentation, you can merge the two compiled html outputs.
For example, to have your project report as the main site on your repository's domain, put the content of your compiled jupyter book (found in `reports/book/_build/html`) in `./docs` (inside the repository's root) and put the Sphinx-compiled code documentation (found in `docsrc/_build/html`) into a subfolder of `.docs/` (e.g. `./docs/code-documentation`). 
Then, your project report will be found on the repository's github page (https://your-github-usernam.github.io/your-project) and your code documentation on https://your-github-usernam.github.io/your-project/code-documentation, respectively.
You could then link your code documentation on your jupyter-book page or make the link somewhere else available.


## Project Structure

    ├── LICENSE            <- The license used for this project
    ├── CHANGELOG.md       <- All major changes should go in there
    ├── Makefile           <- A self-documenting Makefile for standard CLI tasks
    ├── README.md          <- The top-level README of this project
    ├── .env               <- In this file, specify all your custom environment variables
    │                         Keep this out of version control!
    │
    ├── assets             <- A place for assets like shapefiles or config files
    │
    ├── data               <- Contains all data used for the analyses in this project.
    │   │                     The sub-directories can be links to the actual location of your data.
    │   │                     However, they should never be under version control! (-> .gitignore)
    │   ├── interim        <- Intermediate data that have been transformed from the raw data
    │   ├── processed      <- The final, processed data used for the actual analyses
    │   └── raw            <- The original, immutable(!) data
    │
    ├── docsrc             <- The technical documentation (default engine: Sphinx; but feel free to use 
    │                         MkDocs, Jupyter-Book or anything similar).
    │                         This should contain only documentation of the code and the assets.
    │                         A report of the actual project should be placed in `reports/book`.
    │
    ├── logs               <- Storage location for the log files being generated by scripts
    │
    ├── notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
    │   │                     and a short `-` or `_` delimited description, e.g. `01-initial-analyses`
    │   ├── exploratory    <- Notebooks for exploratory tasks
    │   └── reports        <- Notebooks generating reports and figures
    │
    ├── references         <- Data descriptions, manuals, and all other explanatory materials
    │
    ├── reports            <- Generated reports (e.g. HTML, PDF, LaTeX, etc.)
    │   ├── book           <- A Jupyter-Book describing the project
    │   └── figures        <- Generated graphics and figures to be used in reporting
    │
    ├── requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    │                         generated with `pip freeze > requirements.txt`
    │
    ├── setup.py           <- makes project pip installable (pip install -e .) so src can be imported
    ├── scripts            <- High-level scripts that use (low-level) source code from `src/`
    └── src                <- Source code (and only source code!) for use in this project
        ├── tests          <- Contains tests for the code in `src/`
        └── __init__.py    <- Makes src a Python module and provides some standard variables


## Dummy files
The following files are for demonstration purposes only and, if not needed, can be deleted safely:

    ├── notebooks/01-minimal-example.ipynb
    ├── docsrc/*
    ├── reports/book/*
    ├── scripts/01-test.py
    └── src
        ├── tests/*
        └── submodule.py


## Sources of inspiration
Some great sources of inspiration and orientation when I created this template:
- A great article on how to structure your scientific data projects: https://drivendata.github.io/cookiecutter-data-science
- https://github.com/drivendata/cookiecutter-data-science
- https://github.com/audreyfeldroy/cookiecutter-pypackage
- https://github.com/hackalog/easydata
- https://github.com/aubricus/cookiecutter-python-package


## Maintainer
- [Markus Ritschel](https://github.com/markusritschel)

## Contributing
Issues & pull-requests accepted.


---
&copy; Markus Ritschel 2021
